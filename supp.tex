\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

\PassOptionsToPackage{numbers, sort, compress}{natbib}

% ready for submission
\usepackage{neurips_2025}


%\bibliographystyle{abbrvnat}
\bibliographystyle{unsrtnat}


\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\newcommand\reynotes[1]{\textcolor{purple}{#1}}

%\title{SmokeViz: Using Pseudo-Labels to Develop a Human-Labeled Deep Learning Dataset of Wildfire Smoke Plumes in Satellite Imagery}
\title{Supplementary Materials for SmokeViz: A Large-Scale Satellite Dataset for Wildfire Smoke Detection and Segmentation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Rey Koki\\%\thanks{rey.koki@colorado.edu} \\
  Department of Computer Science\\
  University of Colorado Boulder\\
  Boulder, Colorado 80303\\
  \texttt{rey.koki@colorado.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}





\maketitle

\appendix


\section{Original Data and Software Licenses}

The HMS smoke product does not have a license attached to it. For GOES imagery, NOAA states: \emph{"There are no restrictions on the use of this data"}. PyTroll is distributed under the GNU General Public License v3.0, and Segmentation Models PyTorch is distributed under the MIT License.

\section{Satellite and Band Selection}


As described in the main paper, Advanced Baseline Imager (ABI) bands 1-3 were selected for their high signal-to-noise ratio (SNR) and relevance to visible smoke. Figure \ref{G16_vs_G17} shows an example of GOES-West provides higher visibility of a smoke plume than GOES-East near sunrise. This effect is consistent with Mie scattering physics, where forward-scattered light enhances contrast for aerosols like smoke under favorable solar geometry.

\setcounter{figure}{9}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/G17_s20221371250321_35.61_-105.01_36.png}
    \caption{(a) True color GOES-EAST (top) and GOES-WEST (bottom) imagery from May \(18^{th}\), 2022 centered at (\(35.6^{\circ}\), \(-105.0^{\circ}\)) in New Mexico, USA taken at 12:50 UTC. The GOES-East and West raw band imagery for (c) blue, (d) red and (e) veggie bands show variations in the SNR for smoke detection in relation to the \(\lambda\) of light being measured.}\label{G16_vs_G17}
\end{figure}

Figure \ref{all_bands} presents a smoke plume in a cloudy scene across all 16 ABI bands described in Table \ref{band_table}. The smoke signal is prominent in Bands 1–3 but diminishes in subsequent NIR channels. Band C07 (3.9 \(\mu\)m), which is sensitive to thermal anomalies, shows a strong fire signal at the source of the plume. While useful for active fire detection, including C07 for smoke segmentation may bias models toward learning fire-smoke co-location, reducing generalization to detached or low opacity smoke plumes, especially those classified as light density that have traveled far from the source. This concern supports the decision to limit input channels in SmokeViz to those that reflect the analyst operational view while minimizing potential modeling shortcuts and dataset size. The SmokeViz dataset development code is designed to be easily adapted to incorporate any desired spectral bands and/or composites.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/G16_s20221550056176_33_-106_all_band.png}
    \caption{GOES-EAST imagery for all 16 bands from June \(5^{th}\), 2022 centered at (\(33.0^{\circ}\), \(-106.0^{\circ}\)) in New Mexico, USA taken at 00:56 UTC.}\label{all_bands}
\end{figure}

\begin{table}[!htb]
\centering
\caption{The GOES-Series Advanced Baseline Imager (ABI) provides data at 16 channels that cover visible (C01-C02), near-IR (C03-C06) and IR (C07-C16) bands.}
\begin{tabular}{llcc}
\toprule
\textbf{Band} & \textbf{Description} & \textbf{Center Wavelength (\(\mu\)m)} & \textbf{Spatial Resolution (km)} \\
\midrule
C01  & Blue visible                & 0.47     & 1   \\
C02  & Red visible                 & 0.64     & 0.5 \\
C03  & Veggie near IR             & 0.865    & 1   \\
C04  & Cirrus                     & 1.378    & 2   \\
C05  & Snow/Ice                   & 1.61     & 1   \\
C06  & Cloud particle             & 2.24     & 2   \\
C07  & Shortwave IR               & 3.9      & 2   \\
C08  & Upper-level water vapor    & 6.2      & 2   \\
C09  & Mid-level water vapor      & 6.9      & 2   \\
C10  & Lower-level water vapor    & 7.3      & 2   \\
C11  & IR cloud phase             & 8.5      & 2   \\
C12  & Ozone                      & 9.6      & 2   \\
C13  & Clean longwave IR          & 10.35    & 2   \\
C14  & Longwave IR                & 11.2     & 2   \\
C15  & Dirty longwave IR          & 12.3     & 2   \\
C16  & CO\(_2\)                         & 13.3     & 2   \\
\bottomrule
\end{tabular}
\label{band_table}
\end{table}

\section{Statistical Dataset Visualizations}

Figures \ref{num_frames}-\ref{count_per_country} summarize key statistical characteristics of the SmokeViz dataset.

Figure \ref{num_frames} presents a histogram of the number of GOES satellite frames associated with each HMS annotation. Since frames are available every 10 minutes, this visualization reflects the variability in annotation time window duration. Most annotations span between 5 and 50 frames, corresponding to 50 minutes to just over 8 hours, underscoring the need for resolving temporal ambiguity during dataset refinement.

Figure \ref{count_per_yr} shows the number of SmokeViz samples per year, stratified by smoke density. The year 2020 exhibits the highest sample count, aligning with an exceptionally active wildfire season across North America \cite{fires2020}. The density distribution across years also varies, with some years showing a higher relative proportion of light or medium smoke annotations.

Lastly, SmokeViz includes annotations across North America, Figure \ref{count_per_country} summarizes the dataset's geographic coverage by country, including the United States, Canada, and Mexico.



\begin{figure}[!htb]
    \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_vs_frames.png}
        \caption{The number of annotations that span a number of satellite frames that are generated at a 10-minute interval.}
        \label{num_frames}
\end{figure}

\begin{figure}[!htb]
    \parbox{\textwidth}{
      \parbox{0.49\textwidth}{
        \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_per_yr_percentages.png}
        \caption{Annual sample counts in the SmokeViz dataset, broken down by smoke density class. Percentages within each column indicate the relative frequency of each density level for that year.}
        \label{count_per_yr}
      }
    \hspace{0.01\textwidth}
      \parbox{0.49\textwidth}{
        \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_percent_country.png}
        \caption{Proportion of samples in the SmokeViz dataset whose center pixel falls within Canada, the U.S.A., or Mexico. Sample counts are: Canada: 18,461; U.S.: 115,311; Mexico: 25,014. An additional 24,886 samples are centered over other countries or oceans.}
        \label{count_per_country}
      }
    }
\end{figure}

\section{Agricultural Burns}

The monthly peak in sample counts, shown in the main paper (Figure 6), occurs in March and April, preceding the typical wildfire season, which spans from late spring through fall. This early-season spike is likely due to prescribed agricultural burns, which are commonly conducted before vegetation exits winter dormancy \cite{ag_fire}. Since HMS annotations do not distinguish between prescribed burns and wildfires, both event types are included in the dataset.

Model performance over time, also shown in Figure 6, reveals that the highest IoU values for \(f_{c}\) on the \(\mathcal{D}_p\) test set occur during the peak wildfire season (May–September), not during the months with the highest sample counts. This suggests that prescribed burns that are typically smaller and less visually distinct than large wildfires, are more difficult for the model to segment.

A spatial breakdown of sample density further supports this interpretation. Figure 7 in the main paper shows that the states with the highest number of samples are California, Georgia, and Florida. The elevated sample counts in southeastern states are consistent with regional practices of frequent prescribed burns. To investigate regional effects more explicitly, we divide the dataset into four geographic quadrants—Northwest (NW), Southwest (SW), Northeast (NE), and Southeast (SE)—centered around a continental midpoint at \((40^\circ\mathrm{N}, -105^\circ\mathrm{W})\).

Table \ref{quad} reports test IoU and sample counts for each quadrant. Despite containing the largest share of training samples, the Southeast quadrant exhibits the lowest model performance. This degradation is likely due to the abundance of prescribed burns, which may produce smaller, low-opacity smoke plumes that are harder to detect. For users whose objective is to train models specifically for large wildfire detection, this highlights a limitation of the dataset: a substantial portion of the training data originates from controlled burns, which may not be representative of the intended task. One possible mitigation strategy is to filter out short-duration annotations (e.g., single-day events), though this is complicated by variability in analyst-defined time windows and labeling cadence per fire event.

\begin{table}[!htb]
    \caption{SmokeViz dataset sample distribution and \(f_{c}\) test performance across geographic quadrants. Despite containing the most data, the Southeast (SE) region yields the lowest IoU.}
    \label{quad}
    \centering
    \begin{tabular}{lccc}
        \toprule
        Quadrant & SmokeViz Test Set IoU & SmokeViz Test Set Samples & SmokeViz Samples \\
        \midrule
        Northwest (NW) & 0.5887 & 4,177 & 32,792 \\
        Southwest (SW) & 0.4590 & 1,937 & 34,267 \\
        Northeast (NE) & 0.5822 & 1,133 & 13,342 \\
        Southeast (SE) & 0.4798 & 12,977 & 103,271 \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Satellite Analysis}

Figure \ref{sat_dataset} shows the distribution of samples from GOES-East and GOES-West in both the full SmokeViz dataset and the \(\mathcal{D}_p\) test set, along with their respective segmentation performance using \(f_c\). Although GOES-East contributes nearly three-quarters of all samples, model performance is substantially better on GOES-West test samples, with an IoU of 0.6187 compared to 0.4498 for GOES-East.

This discrepancy may stem from several factors. First, the observed signal quality varies between satellites depending on diurnal lighting, seasonal solar angles, and atmospheric conditions. GOES-West best captures forward-scattered sunlight during early morning hours over the western U.S., enhancing smoke visibility via Mie scattering and possibly boosting model accuracy. Additionally, sensor calibration, viewing geometry, and line-of-sight differences between the two platforms could contribute to systematic performance variation.

Another relevant factor is the operational transition between satellites. On June 18, 2022, GOES-17 was replaced by GOES-18 as the operational West satellite. While GOES-17 samples in the test set yield an IoU of 0.6262, GOES-18 samples yield a slightly lower performance at 0.5852. This is likely due to the limited exposure of GOES-18 data during model training: training years (2018–2021) include only GOES-17, while GOES-18 is only present in the 2024 training data. This temporal imbalance may partially explain the drop in IoU for GOES-18.

Overall, these results suggest that while GOES-East offers broader coverage, the more favorable observational geometry of GOES-West—combined with consistent training data—produces stronger segmentation results. Future work may explore satellite-specific fine-tuning or normalization techniques to reduce these performance gaps.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/satellite_test_full_dataset.png}
    \caption{SmokeViz test set (left) and full dataset (right) sample distributions by satellite. GOES-West samples account for a smaller portion of the dataset but yield higher test IoU.}
    \label{sat_dataset}
\end{figure}

\section{Sunset/Sunrise Bias}

As discussed in the main paper's limitations, there exists a potential observational bias toward imagery captured near sunrise or sunset. This bias may originate from both our Mie-derived dataset (\(\mathcal{D}_M\)) and the original HMS annotations. Due to Mie scattering, the sun-satellite-smoke geometry results in a higher signal-to-noise ratio (SNR) when the solar zenith angle is near \(90^{\circ}\), typically around sunrise and sunset. This light configuration enhances the visual detectability of smoke plumes, guiding our initial selection method and potentially the analyst annotations.

In contrast, diurnal fire activity patterns . Wildfires tend to exhibit peak fire radiative power around solar noon due to increased temperature and wind \cite{diurnal}, meaning that smoke intensity and spread are often greatest in midday imagery. The tension between observational clarity and fire behavior complicates dataset curation.

Figure \ref{DM_vs_DP} compares the performance of models trained on \(\mathcal{D}_M\) (left) and the refined PLDR-generated dataset \(\mathcal{D}_p\) (right), segmented by image with temporal proximity to sunrise/sunset versus midday. The Mie-derived dataset favors high-SZA imagery, reflected in stronger IoU for morning/evening frames. In contrast, the SmokeViz dataset (\(\mathcal{D}_p\)), which selects the frame with the best overlap between model prediction and annotation, shows higher IoU for midday images, suggesting it more accurately aligns with fire dynamics rather than SNR optimization bias.

This shift in temporal preference is further quantified in Figure \ref{frame_diff}, which shows the distribution of frame differences between corresponding samples in \(\mathcal{D}_M\) and \(\mathcal{D}_p\). Over 80\% of the annotations were assigned to different satellite frames between the two datasets. This illustrates the degree of temporal refinement enabled by PLDR, which prioritizes semantic alignment over SNR.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/Mie_vs_PL_midday.png}
    \caption{Distribution of test set samples in the Mie-derived dataset (\(\mathcal{D}_M\), left) and SmokeViz dataset (\(\mathcal{D}_p\), right), split by time proximity to sunrise/sunset versus midday. The PLDR refinement leads to large distribution and improved performance for midday samples.}
    \label{DM_vs_DP}
\end{figure}

\begin{figure}[!htb]
    \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_vs_diff_frames.png}
        \caption{Histogram showing the shift in satellite frame selection between the Mie-derived dataset \(\mathcal{D}_M\) and the PLDR-selected SmokeViz dataset \(\mathcal{D}_p\). A value of 0 indicates the same frame was selected. Only 20\% of samples (36,608) used the same frame, indicating substantial temporal reassignment.}
        \label{frame_diff}
\end{figure}



\section{Qualitative Analysis on Performance}

Figure \ref{poor} gives some examples of when SmokeViz does not perform well in comparison to the HMS analyst annotations. In the first column example, SmokeViz confuses a smoke-like cloud for smoke, something it generally tends to not seem to have issues with, but this is an counter example to that trend. The second through last columns all miss distinctive plumes that should be classified as either medium or heavy density smoke. In contrast, we show what SmokeViz looks like when it performs well in comparison to the HMS smoke product in figure \ref{good}.

To complement the quantitative evaluation, we provide qualitative comparisons between the SmokeViz model predictions and the original HMS analyst annotations. These examples illustrate strengths and failure modes observed in the dataset.

Figure \ref{poor} presents five cases where \(f_c\) does not perform optimally. In the first column, the model mistakenly identifies a smoke-like cloud as light-density smoke, a rare misclassification that reveals a potential weakness in cloud/smoke differentiation under certain lighting and texture conditions. In the remaining columns, SmokeViz underestimates the extent or density of visible smoke plumes. These errors generally occur in scenes with faint or fragmented plumes, where the signal-to-noise ratio is lower, or where overlapping atmospheric conditions make segmentation more difficult. In some cases, partial occlusion or lower contrast in the plume edges may have limited the model's confidence.

In contrast, Figure \ref{good} shows examples where SmokeViz closely matches the HMS labels or even outperforms them in delineating plume boundaries. In these scenes, the model produces tighter boundaries that conform well to the visible smoke extent, including fine structural details that the coarser analyst polygons often miss.

Together, these examples show that while the model performs robustly across many diverse smoke scenes, it still faces challenges in edge cases involving ambiguous cloud formations, light plumes, or visually occluded conditions. These qualitative insights can help inform aspects of improvements in future models.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/bad_results.png}
    \caption{Challenging examples where SmokeViz underperforms. Top row: HMS annotations. Bottom row: SmokeViz model predictions. Leftmost column shows a false positive (cloud misclassified as smoke), remaining examples show under-segmentation or missed detection of smoke plumes. IoU values for each column are shown below.}
    \label{poor}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/good_results.png}
    \caption{Examples of successful segmentation by SmokeViz. The model predictions (bottom) align well with HMS annotations (top), capturing plume shape and density more precisely. IoU scores shown below each sample indicate high overlap.}
    \label{good}
\end{figure}


\section{Machine Learning Reproducibility}

All relevant code for dataset construction, model training, and evaluation is publicly available at \url{https://github.com/anonymous-smokeviz/SmokeViz} to ensure transparency and reproducibility.

The models presented in this study are not optimized for state-of-the-art performance but are designed to serve two primary purposes: (1) generate pseudo-labels used in the PLDR workflow to construct the SmokeViz dataset, and (2) evaluate the relative performance of models trained on the Mie-derived dataset (\(\mathcal{D}_M\)) versus the refined SmokeViz dataset (\(\mathcal{D}_p\)).

To select the parent model \(f_{\circ}\), we trained several encoder-decoder architectures on \(\mathcal{D}_M\) and selected the one that achieved the highest overall Intersection over Union (IoU), as shown in Table \ref{child}. The top-performing model used EfficientNetV2 as the encoder and PSPNet as the decoder. This architecture was then used to generate intermediary pseudo-labels for the PLDR process.

\begin{table}[!htb] 
    \caption{Comparison of segmentation model IoU metrics on the Mie-derived dataset (\(\mathcal{D}_M\)). The highest overall IoU model (EfficientNetV2 + PSPNet) was selected as \(f_{\circ}\).}
    \centering
    \begin{tabular}{lccccc}
        \toprule
        \textbf{encoder} & EfficientNet\cite{efficientnetv2} & \cite{efficientnetv2} & EfficientViT\cite{efficientvit} & \cite{efficientvit}& ViT \cite{vit} \\
        \textbf{decoder} &  PSPNet\cite{pspnet} & DeepLabV3+\cite{deeplab} & Segformer\cite{segformer} & UperNet \cite{upernet}& DPT\cite{dpt} \\
        \midrule
        Heavy   &	\textbf{0.3221}	& 0.2893	&	0.2185	&	0.3099	&	0.2466 \\
        Medium  &	\textbf{0.4288}	& 0.4091	&	0.3977	&   0.4041	&	0.3135 \\
        Light   &	\textbf{0.5044}	& 0.4424	&	0.4331	&	0.4274	&	0.4964 \\
        Overall &	\textbf{0.4677}	& 0.4172	&   0.4054	&	0.4098	&	0.4331 \\
        \bottomrule
    \end{tabular}
    \label{child}
\end{table}

All models were trained using the Adam optimizer with a learning rate of \(1 \times 10^{-4}\), batch size of 16, and 100 training epochs. Hyperparameter values were chosen based on memory constraints and general suitability for large-scale semantic segmentation tasks. 

\pagebreak

\subsection{Datasheet for SmokeViz}

Questions from the https://arxiv.org/abs/1803.09010 paper, v7.

\subsubsection{Motivation}

The questions in this section are primarily intended to encourage dataset creators to clearly articulate their reasons for creating the dataset and to promote transparencyabout funding interests.

\textbf{For what purpose was the dataset created? }

SmokeViz was created to serve as a large labeled dataset to be used in creating wildfire smoke plume related machine learning models. Applications include wildfire smoke detection or smoke dispersion modeling.

\textbf{Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?}

REMOVED TO KEEP ANONYMITY DURING REVIEW PROCESS.

\textbf{Who funded the creation of the dataset?} 

REMOVED TO KEEP ANONYMITY DURING REVIEW PROCESS.

\textbf{Any other comments?}

None.

\subsubsection{Composition}

Most of these questions are intended to provide dataset consumers with the information they need to make informed decisions about using the dataset for specific tasks. The answers to some of these questions reveal information about compliance with the EU’s General Data Protection Regulation (GDPR) or comparable regulations in other jurisdictions.

\textbf{What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?}

Each instance is a 256x256x3 RGB image from GOES imagery with an accompanying 256x256x3 binary masks corresponding to density of smoke. There are 3 densities of smoke - Light, Medium and Heavy.


\textbf{How many instances are there in total (of each type, if appropriate)?} 

There are 183,672 samples - 128,755 for light, 35,710 for medium and 19,207 for heavy density smoke.


\textbf{Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? }

The entire possible number of samples between 2018/01/01 - 2024/11/01 is 210,702. The dataset is reduced to 207,106 samples after filtering out any samples with no corresponding satellite imagery available or imagery that is less than 10 or over 90 percent saturation. Total saturation is defined when each pixel is equal to 1. Mentioned in more detail in the paper, the dataset was further reduced down to 183,672 samples after applying a .01 IoU threshold during the pseudo-labeling process.

\textbf{What data does each instance consist of? }

The data is processed to correct for Rayleigh scattering, solar zenith angle and projected so each pixel is representative of the same area of land. The algorithm is referenced in the SmokeViz paper.

\textbf{Is there a label or target associated with each instance?}

Yes, there are no samples that are intended to not display any smoke.

\textbf{Is any information missing from individual instances?}

We have seen imagery where smoke is labeled but there's adjacent smoke plumes that were unlabeled. With human labels comes human errors.

\textbf{Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)?}

Some instances can overap in geographic location, there can be multiple smoke plumes in one instance, but the index of the HMS smoke annotation is listed and can be mapped back to the original dataset for geolocational information.

\textbf{Are there recommended data splits (e.g., training, development/validation, testing)?}

We recommend using full years of data for training, validation and testing to keep full year long patterns of wildfire behavior. We use 2018-2021 and 2024 for training, 2023 for validation and 2022 for testing.

\textbf{Are there any errors, sources of noise, or redundancies in the dataset?}

The HMS smoke annotations that are used as truth are a source of noise as explained in the SmokeViz paper. These include approximations of smoke polygons mismatching actual location and time windows being too large that smoke moves during the time window. There is also noise caused by atmospheric interactions with light. Redundancies occur when there more than one smoke plume and annotation in one image.

\textbf{Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?}

The dataset is self-contained.

\textbf{Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals’ non-public communications)?}

No.

\textbf{Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?}

No.

\textbf{Does the dataset relate to people? }

No, not directly, although wildfires do affect people, these images are at 1km resolution and do not show enough detail to relate to people or infrastructure.

\textbf{Does the dataset identify any subpopulations (e.g., by age, gender)?}

No.

\textbf{Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?}

No.

\textbf{Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?}

No.

\textbf{Any other comments?}

No.


\subsubsection{Collection process}

The answers to questions here may provide information that allow others to reconstruct the dataset without access to it.

\textbf{How was the data associated with each instance acquired?}

The labels from HMS smoke product are not validated or verified but is used for AirNow air quality assessments. The GOES imagery is collected by the ABI sensor and is corrected for any anomalies and also converted from photon count to radiance values.

\textbf{What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?}

Original low temporal resolution annotations were manual human analyst curated. To create the high temporal resolution annotations, we use pseudo-labeling discussed in detail within the SmokeViz paper.

\textbf{If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?}

The HMS smoke analysts are only looking for smoke during the daytime and do avoid annotations during heavy cloud cover.

\textbf{Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?}

The NOAA NESDIS employed analysts are compensated as salaried federal employees.

\textbf{Over what timeframe was the data collected?}

2018-2024

\textbf{Were any ethical review processes conducted (e.g., by an institutional review board)?}

No.


\subsubsection{Preprocessing/cleaning/labeling}


The questions in this section are intended to provide dataset consumers with the information they need to determine whether the “raw” data has been processed in ways that are compatible with their chosen tasks. For example, text that has been converted into a “bag-of-words” is not suitable for tasks involving word order.

\textbf{Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?}

The data was processed according to the GOES True Color paper referenced in the SmokeViz paper methods section. This includes atmospheric, Rayleigh corrections and estimation of a Green band.

\textbf{Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?}

The raw data is available from the NOAA AWS webpage.
\url{https://registry.opendata.aws/noaa-goes/}
The HMS smoke annotations are available here:
\url{https://www.ospo.noaa.gov/products/land/hms.html}

\textbf{Is the software used to preprocess/clean/label the instances available?}


Yes, Pytroll implements the algorithm discussed in the GOES True Color paper referenced in the SmokeViz paper.

\textbf{Any other comments?}
None.

\subsubsection{Uses}

These questions are intended to encourage dataset creators to reflect on the tasks for which the dataset should and should not be used. By explicitly highlighting these tasks, dataset creators can help dataset consumers to make informed decisions, thereby avoiding potential risks or harms.

\textbf{Has the dataset been used for any tasks already?}

It was used to train benchmark models mentioned in the paper that apply semantic segmentation to identify and classify smoke in satellite imagery.

\textbf{Is there a repository that links to any or all papers or systems that use the dataset?}

No.

\textbf{What (other) tasks could the dataset be used for?}

A machine learning based smoke dispersion forecast model, automated wildfire smoke detection and segementation, a smoke analysis product for data assimilation into smoke or air quality models.

\textbf{Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?}
No.

\textbf{Are there tasks for which the dataset should not be used?}
No.

\textbf{Any other comments?}
None

\subsubsection{Distribution}

\textbf{Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? }

No.

\textbf{How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?}

\url{https://noaa-gsl-experimental-pds.s3.amazonaws.com/index.html#SmokeViz/}

\textbf{When will the dataset be distributed?}

It is currently available.

\textbf{Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?}

No.

\textbf{Have any third parties imposed IP-based or other restrictions on the data associated with the instances?}

No.

\textbf{Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?}

No.

\textbf{Any other comments?}

None.

\subsubsection{Maintenance}

These questions are intended to encourage dataset creators to plan for dataset maintenance and communicate this plan with dataset consumers.

\textbf{Who is supporting/hosting/maintaining the dataset?}

National Oceanic and Atmospheric Administration Global Systems Laboratory is hosting the dataset on Amazon Web Services.

\textbf{How can the owner/curator/manager of the dataset be contacted (e.g., email address)?}

REMOVED TO KEEP ANONYMITY DURING REVIEW PROCESS.

\textbf{Is there an erratum?}

No.

\textbf{Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?}

Yes, only to add new instances.

\textbf{If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?}

Not applicable.

\textbf{Will older versions of the dataset continue to be supported/hosted/maintained?}

No, it is too large to keep multple versions.

\textbf{If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?}

The code to extend/augment/build is publicly available \url{https://github.com/anonymous-smokeviz/SmokeViz}. We encourage anyone that would like to contribute to SmokeViz to reach out to REMOVED TO KEEP ANONYMITY DURING REVIEW PROCESS.

\textbf{Any other comments?}

None



\bibliography{references}
\end{document}

