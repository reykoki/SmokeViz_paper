\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

\PassOptionsToPackage{numbers, sort, compress}{natbib}

% ready for submission
\usepackage{neurips_2025}


%\bibliographystyle{abbrvnat}
\bibliographystyle{unsrtnat}


\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\newcommand\reynotes[1]{\textcolor{purple}{#1}}

%\title{SmokeViz: Using Pseudo-Labels to Develop a Human-Labeled Deep Learning Dataset of Wildfire Smoke Plumes in Satellite Imagery}
\title{Supplementary Materials for SmokeViz: A Large-Scale Satellite Dataset for Wildfire Smoke Detection and Segmentation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Rey Koki\\%\thanks{rey.koki@colorado.edu} \\
  Department of Computer Science\\
  University of Colorado Boulder\\
  Boulder, Colorado 80303\\
  \texttt{rey.koki@colorado.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}





\maketitle

\appendix


\section{Original Data and Software Licenses}

The HMS smoke product does not have a license attached to it. For GOES imagery, NOAA states: \emph{"There are no restrictions on the use of this data"}. PyTroll is distributed under the GNU General Public License v3.0, and Segmentation Models PyTorch is distributed under the MIT License.

\section{Satellite and Band Selection}


As described in the main paper, Advanced Baseline Imager (ABI) bands 1-3 were selected for their high signal-to-noise ratio (SNR) and relevance to visible smoke. Figure \ref{G16_vs_G17} shows an example of GOES-West provides higher visibility of a smoke plume than GOES-East near sunrise. This effect is consistent with Mie scattering physics, where forward-scattered light enhances contrast for aerosols like smoke under favorable solar geometry.

\setcounter{figure}{9}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/G17_s20221371250321_35.61_-105.01_36.png}
    \caption{(a) True color GOES-EAST (top) and GOES-WEST (bottom) imagery from May \(18^{th}\), 2022 centered at (\(35.6^{\circ}\), \(-105.0^{\circ}\)) in New Mexico, USA taken at 12:50 UTC. The GOES-East and West raw band imagery for (c) blue, (d) red and (e) veggie bands show variations in the SNR for smoke detection in relation to the \(\lambda\) of light being measured.}\label{G16_vs_G17}
\end{figure}

Figure \ref{all_bands} presents a smoke plume in a cloudy scene across all 16 ABI bands described in Table \ref{band_table}. The smoke signal is prominent in Bands 1–3 but diminishes in subsequent NIR channels. Band C07 (3.9 \(\mu\)m), which is sensitive to thermal anomalies, shows a strong fire signal at the source of the plume. While useful for active fire detection, including C07 for smoke segmentation may bias models toward learning fire-smoke co-location, reducing generalization to detached or low opacity smoke plumes, especially those classified as light density that have traveled far from the source. This concern supports the decision to limit input channels in SmokeViz to those that reflect the analyst operational view while minimizing potential modeling shortcuts and dataset size. The SmokeViz dataset development code is designed to be easily adapted to incorporate any desired spectral bands and/or composites.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/G16_s20221550056176_33_-106_all_band.png}
    \caption{GOES-EAST imagery for all 16 bands from June \(5^{th}\), 2022 centered at (\(33.0^{\circ}\), \(-106.0^{\circ}\)) in New Mexico, USA taken at 00:56 UTC.}\label{all_bands}
\end{figure}

\begin{table}[!htb]
\centering
\caption{The GOES-Series Advanced Baseline Imager (ABI) provides data at 16 channels that cover visible (C01-C02), near-IR (C03-C06) and IR (C07-C16) bands.}
\begin{tabular}{llcc}
\toprule
\textbf{Band} & \textbf{Description} & \textbf{Center Wavelength (\(\mu\)m)} & \textbf{Spatial Resolution (km)} \\
\midrule
C01  & Blue visible                & 0.47     & 1   \\
C02  & Red visible                 & 0.64     & 0.5 \\
C03  & Veggie near IR             & 0.865    & 1   \\
C04  & Cirrus                     & 1.378    & 2   \\
C05  & Snow/Ice                   & 1.61     & 1   \\
C06  & Cloud particle             & 2.24     & 2   \\
C07  & Shortwave IR               & 3.9      & 2   \\
C08  & Upper-level water vapor    & 6.2      & 2   \\
C09  & Mid-level water vapor      & 6.9      & 2   \\
C10  & Lower-level water vapor    & 7.3      & 2   \\
C11  & IR cloud phase             & 8.5      & 2   \\
C12  & Ozone                      & 9.6      & 2   \\
C13  & Clean longwave IR          & 10.35    & 2   \\
C14  & Longwave IR                & 11.2     & 2   \\
C15  & Dirty longwave IR          & 12.3     & 2   \\
C16  & CO\(_2\)                         & 13.3     & 2   \\
\bottomrule
\end{tabular}
\label{band_table}
\end{table}

\section{Statistical Dataset Visualizations}

Figures \ref{num_frames}-\ref{count_per_country} summarize key statistical characteristics of the SmokeViz dataset.

Figure \ref{num_frames} presents a histogram of the number of GOES satellite frames associated with each HMS annotation. Since frames are available every 10 minutes, this visualization reflects the variability in annotation time window duration. Most annotations span between 5 and 50 frames, corresponding to 50 minutes to just over 8 hours, underscoring the need for resolving temporal ambiguity during dataset refinement.

Figure \ref{count_per_yr} shows the number of SmokeViz samples per year, stratified by smoke density. The year 2020 exhibits the highest sample count, aligning with an exceptionally active wildfire season across North America \cite{fires2020}. The density distribution across years also varies, with some years showing a higher relative proportion of light or medium smoke annotations.

Lastly, SmokeViz includes annotations across North America, Figure \ref{count_per_country} summarizes the dataset's geographic coverage by country, including the United States, Canada, and Mexico.



\begin{figure}[!htb]
    \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_vs_frames.png}
        \caption{The number of annotations that span a number of satellite frames that are generated at a 10-minute interval.}
        \label{num_frames}
\end{figure}

\begin{figure}[!htb]
    \parbox{\textwidth}{
      \parbox{0.49\textwidth}{
        \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_per_yr_percentages.png}
        \caption{Annual sample counts in the SmokeViz dataset, broken down by smoke density class. Percentages within each column indicate the relative frequency of each density level for that year.}
        \label{count_per_yr}
      }
    \hspace{0.01\textwidth}
      \parbox{0.49\textwidth}{
        \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_percent_country.png}
        \caption{Proportion of samples in the SmokeViz dataset whose center pixel falls within Canada, the U.S.A., or Mexico. Sample counts are: Canada: 18,461; U.S.: 115,311; Mexico: 25,014. An additional 24,886 samples are centered over other countries or oceans.}
        \label{count_per_country}
      }
    }
\end{figure}

\section{Agricultural Burns}

The monthly peak in sample counts, shown in the main paper (Figure 6), occurs in March and April, preceding the typical wildfire season, which spans from late spring through fall. This early-season spike is likely due to prescribed agricultural burns, which are commonly conducted before vegetation exits winter dormancy \cite{ag_fire}. Since HMS annotations do not distinguish between prescribed burns and wildfires, both event types are included in the dataset.

Model performance over time, also shown in Figure 6, reveals that the highest IoU values for \(f_{c}\) on the \(\mathcal{D}_p\) test set occur during the peak wildfire season (May–September), not during the months with the highest sample counts. This suggests that prescribed burns that are typically smaller and less visually distinct than large wildfires, are more difficult for the model to segment.

A spatial breakdown of sample density further supports this interpretation. Figure 7 in the main paper shows that the states with the highest number of samples are California, Georgia, and Florida. The elevated sample counts in southeastern states are consistent with regional practices of frequent prescribed burns. To investigate regional effects more explicitly, we divide the dataset into four geographic quadrants—Northwest (NW), Southwest (SW), Northeast (NE), and Southeast (SE)—centered around a continental midpoint at \((40^\circ\mathrm{N}, -105^\circ\mathrm{W})\).

Table \ref{quad} reports test IoU and sample counts for each quadrant. Despite containing the largest share of training samples, the Southeast quadrant exhibits the lowest model performance. This degradation is likely due to the abundance of prescribed burns, which may produce smaller, low-opacity smoke plumes that are harder to detect. For users whose objective is to train models specifically for large wildfire detection, this highlights a limitation of the dataset: a substantial portion of the training data originates from controlled burns, which may not be representative of the intended task. One possible mitigation strategy is to filter out short-duration annotations (e.g., single-day events), though this is complicated by variability in analyst-defined time windows and labeling cadence per fire event.

\begin{table}[!htb]
    \caption{SmokeViz dataset sample distribution and \(f_{c}\) test performance across geographic quadrants. Despite containing the most data, the Southeast (SE) region yields the lowest IoU.}
    \label{quad}
    \centering
    \begin{tabular}{lccc}
        \toprule
        Quadrant & SmokeViz Test Set IoU & SmokeViz Test Set Samples & SmokeViz Samples \\
        \midrule
        Northwest (NW) & 0.5887 & 4,177 & 32,792 \\
        Southwest (SW) & 0.4590 & 1,937 & 34,267 \\
        Northeast (NE) & 0.5822 & 1,133 & 13,342 \\
        Southeast (SE) & 0.4798 & 12,977 & 103,271 \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Satellite Analysis}

Figure \ref{sat_dataset} shows the distribution of samples from GOES-East and GOES-West in both the full SmokeViz dataset and the \(\mathcal{D}_p\) test set, along with their respective segmentation performance using \(f_c\). Although GOES-East contributes nearly three-quarters of all samples, model performance is substantially better on GOES-West test samples, with an IoU of 0.6187 compared to 0.4498 for GOES-East.

This discrepancy may stem from several factors. First, the observed signal quality varies between satellites depending on diurnal lighting, seasonal solar angles, and atmospheric conditions. GOES-West best captures forward-scattered sunlight during early morning hours over the western U.S., enhancing smoke visibility via Mie scattering and possibly boosting model accuracy. Additionally, sensor calibration, viewing geometry, and line-of-sight differences between the two platforms could contribute to systematic performance variation.

Another relevant factor is the operational transition between satellites. On June 18, 2022, GOES-17 was replaced by GOES-18 as the operational West satellite. While GOES-17 samples in the test set yield an IoU of 0.6262, GOES-18 samples yield a slightly lower performance at 0.5852. This is likely due to the limited exposure of GOES-18 data during model training: training years (2018–2021) include only GOES-17, while GOES-18 is only present in the 2024 training data. This temporal imbalance may partially explain the drop in IoU for GOES-18.

Overall, these results suggest that while GOES-East offers broader coverage, the more favorable observational geometry of GOES-West—combined with consistent training data—produces stronger segmentation results. Future work may explore satellite-specific fine-tuning or normalization techniques to reduce these performance gaps.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/satellite_test_full_dataset.png}
    \caption{SmokeViz test set (left) and full dataset (right) sample distributions by satellite. GOES-West samples account for a smaller portion of the dataset but yield higher test IoU.}
    \label{sat_dataset}
\end{figure}

\section{Sunset/Sunrise Bias}

As discussed in the main paper's limitations, there exists a potential observational bias toward imagery captured near sunrise or sunset. This bias may originate from both our Mie-derived dataset (\(\mathcal{D}_M\)) and the original HMS annotations. Due to Mie scattering, the sun-satellite-smoke geometry results in a higher signal-to-noise ratio (SNR) when the solar zenith angle is near \(90^{\circ}\), typically around sunrise and sunset. This light configuration enhances the visual detectability of smoke plumes, guiding our initial selection method and potentially the analyst annotations.

In contrast, diurnal fire activity patterns . Wildfires tend to exhibit peak fire radiative power around solar noon due to increased temperature and wind \cite{diurnal}, meaning that smoke intensity and spread are often greatest in midday imagery. The tension between observational clarity and fire behavior complicates dataset curation.

Figure \ref{DM_vs_DP} compares the performance of models trained on \(\mathcal{D}_M\) (left) and the refined PLDR-generated dataset \(\mathcal{D}_p\) (right), segmented by image with temporal proximity to sunrise/sunset versus midday. The Mie-derived dataset favors high-SZA imagery, reflected in stronger IoU for morning/evening frames. In contrast, the SmokeViz dataset (\(\mathcal{D}_p\)), which selects the frame with the best overlap between model prediction and annotation, shows higher IoU for midday images, suggesting it more accurately aligns with fire dynamics rather than SNR optimization bias.

This shift in temporal preference is further quantified in Figure \ref{frame_diff}, which shows the distribution of frame differences between corresponding samples in \(\mathcal{D}_M\) and \(\mathcal{D}_p\). Over 80\% of the annotations were assigned to different satellite frames between the two datasets. This illustrates the degree of temporal refinement enabled by PLDR, which prioritizes semantic alignment over SNR.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/Mie_vs_PL_midday.png}
    \caption{Distribution of test set samples in the Mie-derived dataset (\(\mathcal{D}_M\), left) and SmokeViz dataset (\(\mathcal{D}_p\), right), split by time proximity to sunrise/sunset versus midday. The PLDR refinement leads to large distribution and improved performance for midday samples.}
    \label{DM_vs_DP}
\end{figure}

\begin{figure}[!htb]
    \centering
        \includegraphics[width=0.49\textwidth]{stat_figs/sample_count_vs_diff_frames.png}
        \caption{Histogram showing the shift in satellite frame selection between the Mie-derived dataset \(\mathcal{D}_M\) and the PLDR-selected SmokeViz dataset \(\mathcal{D}_p\). A value of 0 indicates the same frame was selected. Only 20\% of samples (36,608) used the same frame, indicating substantial temporal reassignment.}
        \label{frame_diff}
\end{figure}



\section{Qualitative Analysis on Performance}

Figure \ref{poor} gives some examples of when SmokeViz does not perform well in comparison to the HMS analyst annotations. In the first column example, SmokeViz confuses a smoke-like cloud for smoke, something it generally tends to not seem to have issues with, but this is an counter example to that trend. The second through last columns all miss distinctive plumes that should be classified as either medium or heavy density smoke. In contrast, we show what SmokeViz looks like when it performs well in comparison to the HMS smoke product in figure \ref{good}.

To complement the quantitative evaluation, we provide qualitative comparisons between the SmokeViz model predictions and the original HMS analyst annotations. These examples illustrate strengths and failure modes observed in the dataset.

Figure \ref{poor} presents five cases where \(f_c\) does not perform optimally. In the first column, the model mistakenly identifies a smoke-like cloud as light-density smoke, a rare misclassification that reveals a potential weakness in cloud/smoke differentiation under certain lighting and texture conditions. In the remaining columns, SmokeViz underestimates the extent or density of visible smoke plumes. These errors generally occur in scenes with faint or fragmented plumes, where the signal-to-noise ratio is lower, or where overlapping atmospheric conditions make segmentation more difficult. In some cases, partial occlusion or lower contrast in the plume edges may have limited the model's confidence.

In contrast, Figure \ref{good} shows examples where SmokeViz closely matches the HMS labels or even outperforms them in delineating plume boundaries. In these scenes, the model produces tighter boundaries that conform well to the visible smoke extent, including fine structural details that the coarser analyst polygons often miss.

Together, these examples show that while the model performs robustly across many diverse smoke scenes, it still faces challenges in edge cases involving ambiguous cloud formations, light plumes, or visually occluded conditions. These qualitative insights can help inform aspects of improvements in future models.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/bad_results.png}
    \caption{Challenging examples where SmokeViz underperforms. Top row: HMS annotations. Bottom row: SmokeViz model predictions. Leftmost column shows a false positive (cloud misclassified as smoke), remaining examples show under-segmentation or missed detection of smoke plumes. IoU values for each column are shown below.}
    \label{poor}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{stat_figs/good_results.png}
    \caption{Examples of successful segmentation by SmokeViz. The model predictions (bottom) align well with HMS annotations (top), capturing plume shape and density more precisely. IoU scores shown below each sample indicate high overlap.}
    \label{good}
\end{figure}


\section{Machine Learning Reproducibility}

All relevant code is accessible at \url{https://github.com/anonymous-smokeviz/SmokeViz}. The models presented in this paper are not optimized for performance, but are intended to create sufficient pseudo-labels to develop the SmokeViz dataset and then compare the performance of SmokeViz against the original dataset. The child model used in the PLDR process was chosen by running the models shown in table \ref{child} on the Mie-derived dataset and choosing the model with the highest IoU. The hyperparameters used in for the models are shown in table \ref{hyper}. We use the Adam optimizer that will adapt the learning rate during training and is suited for problems with large amounts of data. Batch size was chosen due to the necessity to run the model on limited resources. 

\begin{table}[!htb] 
    \caption{Comparison of segmentation model IoU metrics on the Mie-Derived dataset. Based on the IoU results, the first column that uses EfficientNet and PSPNet was used as for \(f_{\circ}\).}\label{child}
    \centering
    \begin{tabular}{lccccc}
        \toprule
        \textbf{encoder} & EfficientNet\cite{efficientnetv2} & \cite{efficientnetv2} & EfficientViT\cite{efficientvit} & \cite{efficientvit}& ViT \cite{vit} \\
        \textbf{decoder} &  PSPNet\cite{pspnet} & DeepLabV3+\cite{deeplab} & Segformer\cite{segformer} & UperNet \cite{upernet}& DPT\cite{dpt} \\
        \midrule
        Heavy   &	\textbf{0.3221}	& 0.2893	&	0.2185	&	0.3099	&	0.2466 \\
        Medium  &	\textbf{0.4288}	& 0.4091	&	0.3977	&   0.4041	&	0.3135 \\
        Light   &	\textbf{0.5044}	& 0.4424	&	0.4331	&	0.4274	&	0.4964 \\
        Overall &	\textbf{0.4677}	& 0.4172	&   0.4054	&	0.4098	&	0.4331 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \caption{Hyperparameters used to create \(f_{\circ}\) and \(f_{c}\).}
  \label{hyper}
  \centering
  \begin{tabular}{ll}
    \toprule
    parameter & value \\ 
    \midrule
    epochs & 100 \\
    learning rate & 1e-4 \\
    batch size & 16 \\
    optimizer & Adam \\
    \bottomrule
  \end{tabular}
\end{table}


\bibliography{references}
\end{document}
\pagebreak

